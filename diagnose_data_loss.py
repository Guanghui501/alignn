#!/usr/bin/env python
"""
æ•°æ®ä¸¢å¤±è¯Šæ–­å·¥å…·
æ£€æŸ¥ä¸ºä»€ä¹ˆæ•°æ®ä» 83k+ é™åˆ°åªæœ‰å‡ ä¸ªæ ·æœ¬
"""
import os
import sys
import glob
import pandas as pd
from pathlib import Path

def diagnose_data_loss(data_dir):
    """è¯Šæ–­æ•°æ®ä¸¢å¤±é—®é¢˜"""

    print("=" * 70)
    print("æ•°æ®ä¸¢å¤±è¯Šæ–­å·¥å…·")
    print("=" * 70)
    print(f"\næ•°æ®ç›®å½•: {data_dir}\n")

    # 1. æ£€æŸ¥ id_prop.csv
    print("1ï¸âƒ£  æ£€æŸ¥ id_prop.csv")
    print("-" * 70)

    csv_path = os.path.join(data_dir, "id_prop.csv")

    if os.path.exists(csv_path):
        try:
            df = pd.read_csv(csv_path)
            print(f"  å½“å‰è®°å½•æ•°: {len(df)}")
            print(f"  åˆ—å: {list(df.columns)}")

            if len(df) < 20:
                print(f"\n  å®Œæ•´å†…å®¹:")
                print(df.to_string(index=False))
        except Exception as e:
            print(f"  âŒ è¯»å–é”™è¯¯: {e}")
    else:
        print(f"  âŒ æ–‡ä»¶ä¸å­˜åœ¨: {csv_path}")

    # 2. æ£€æŸ¥æ‰€æœ‰å¤‡ä»½æ–‡ä»¶
    print(f"\n2ï¸âƒ£  æ£€æŸ¥å¤‡ä»½æ–‡ä»¶")
    print("-" * 70)

    backup_patterns = [
        "id_prop.csv.backup*",
        "id_prop.csv.bak*",
        "*.backup",
    ]

    all_backups = []
    for pattern in backup_patterns:
        backups = glob.glob(os.path.join(data_dir, pattern))
        all_backups.extend(backups)

    if all_backups:
        print(f"  æ‰¾åˆ° {len(all_backups)} ä¸ªå¤‡ä»½æ–‡ä»¶:\n")
        for backup in sorted(all_backups):
            fname = os.path.basename(backup)
            size = os.path.getsize(backup)

            # å°è¯•è¯»å–è¡Œæ•°
            try:
                if 'csv' in fname:
                    with open(backup, 'r') as f:
                        lines = sum(1 for _ in f)
                    print(f"  ğŸ“„ {fname:40s} | {size:>10,} bytes | {lines:>7,} è¡Œ")
                else:
                    print(f"  ğŸ“„ {fname:40s} | {size:>10,} bytes")
            except:
                print(f"  ğŸ“„ {fname:40s} | {size:>10,} bytes")
    else:
        print("  âš ï¸  æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½æ–‡ä»¶")

    # 3. æ£€æŸ¥ CIF æ–‡ä»¶
    print(f"\n3ï¸âƒ£  æ£€æŸ¥ CIF æ–‡ä»¶")
    print("-" * 70)

    cif_files = glob.glob(os.path.join(data_dir, "*.cif"))
    print(f"  å½“å‰ç›®å½• CIF æ–‡ä»¶æ•°: {len(cif_files)}")

    if len(cif_files) < 20:
        print(f"\n  æ–‡ä»¶åˆ—è¡¨:")
        for f in sorted(cif_files):
            print(f"    - {os.path.basename(f)}")

    # 4. æ£€æŸ¥ bad_cif_files ç›®å½•
    print(f"\n4ï¸âƒ£  æ£€æŸ¥ bad_cif_files ç›®å½•")
    print("-" * 70)

    bad_dir = os.path.join(data_dir, "bad_cif_files")
    if os.path.exists(bad_dir):
        bad_files = glob.glob(os.path.join(bad_dir, "*.cif"))
        print(f"  ç§»åŠ¨çš„æ–‡ä»¶æ•°: {len(bad_files)}")

        if len(bad_files) > 0:
            print(f"  âš ï¸  è­¦å‘Š: {len(bad_files)} ä¸ªæ–‡ä»¶è¢«ç§»åŠ¨åˆ°æ­¤ç›®å½•")

            # æ£€æŸ¥ bad_cif_files.txt
            txt_file = os.path.join(data_dir, "bad_cif_files.txt")
            if os.path.exists(txt_file):
                print(f"\n  é—®é¢˜æ–‡ä»¶åˆ—è¡¨: {txt_file}")
                with open(txt_file, 'r') as f:
                    lines = f.readlines()[:10]
                    for line in lines:
                        if not line.startswith('#'):
                            print(f"    {line.strip()}")
    else:
        print(f"  ç›®å½•ä¸å­˜åœ¨")

    # 5. æœç´¢å…¶ä»–å¯èƒ½çš„ä½ç½®
    print(f"\n5ï¸âƒ£  æœç´¢å…¶ä»–ä½ç½®çš„ CIF æ–‡ä»¶")
    print("-" * 70)

    parent_dir = os.path.dirname(data_dir)
    all_cif = glob.glob(os.path.join(parent_dir, "**/*.cif"), recursive=True)

    # æŒ‰ç›®å½•ç»Ÿè®¡
    dir_counts = {}
    for f in all_cif:
        d = os.path.dirname(f)
        dir_counts[d] = dir_counts.get(d, 0) + 1

    if dir_counts:
        print(f"  æ‰¾åˆ°çš„ CIF æ–‡ä»¶åˆ†å¸ƒ:\n")
        for d, count in sorted(dir_counts.items(), key=lambda x: -x[1]):
            rel_path = os.path.relpath(d, parent_dir)
            print(f"    {rel_path:50s} : {count:>7,} æ–‡ä»¶")

    # 6. å»ºè®®
    print(f"\n" + "=" * 70)
    print("ğŸ’¡ è¯Šæ–­ç»“æœå’Œå»ºè®®")
    print("=" * 70)

    current_csv_lines = 0
    if os.path.exists(csv_path):
        try:
            df = pd.read_csv(csv_path)
            current_csv_lines = len(df)
        except:
            pass

    current_cif_count = len(cif_files)
    bad_cif_count = len(glob.glob(os.path.join(bad_dir, "*.cif"))) if os.path.exists(bad_dir) else 0

    # æ‰¾åˆ°æœ€å¤§çš„å¤‡ä»½
    largest_backup = None
    largest_backup_lines = 0

    for backup in all_backups:
        if 'csv' in os.path.basename(backup):
            try:
                with open(backup, 'r') as f:
                    lines = sum(1 for _ in f)
                if lines > largest_backup_lines:
                    largest_backup_lines = lines
                    largest_backup = backup
            except:
                pass

    print(f"\nå½“å‰çŠ¶æ€:")
    print(f"  - id_prop.csv: {current_csv_lines} è¡Œ")
    print(f"  - CIF æ–‡ä»¶: {current_cif_count} ä¸ª")
    print(f"  - bad_cif_files: {bad_cif_count} ä¸ª")

    if largest_backup:
        print(f"\næœ€å¤§çš„å¤‡ä»½:")
        print(f"  - æ–‡ä»¶: {os.path.basename(largest_backup)}")
        print(f"  - è¡Œæ•°: {largest_backup_lines}")
        print(f"  - ä¸¢å¤±: {largest_backup_lines - current_csv_lines} è¡Œ")

    # æä¾›æ¢å¤å»ºè®®
    print(f"\nğŸ”§ æ¢å¤å»ºè®®:")

    if largest_backup and largest_backup_lines > current_csv_lines * 10:
        print(f"\nâœ… æ–¹æ¡ˆ1: æ¢å¤æœ€å¤§çš„å¤‡ä»½ï¼ˆæ¨èï¼‰")
        print(f"   cp {largest_backup} {csv_path}")
        print(f"   python fix_id_prop_csv.py {csv_path}")
        print(f"   # é¢„è®¡æ¢å¤ {largest_backup_lines} è¡Œæ•°æ®")

    if bad_cif_count > current_cif_count:
        print(f"\nâœ… æ–¹æ¡ˆ2: æ¢å¤ bad_cif_files ä¸­çš„æ–‡ä»¶")
        print(f"   mv {bad_dir}/*.cif {data_dir}/")
        print(f"   # æ¢å¤ {bad_cif_count} ä¸ª CIF æ–‡ä»¶")

    # æŸ¥æ‰¾å…¶ä»–å¯èƒ½çš„æ•°æ®æº
    other_large_dirs = [d for d, c in dir_counts.items()
                       if c > current_cif_count and d != data_dir]

    if other_large_dirs:
        print(f"\nâœ… æ–¹æ¡ˆ3: ä»å…¶ä»–ç›®å½•å¤åˆ¶")
        for d in other_large_dirs[:3]:
            count = dir_counts[d]
            rel_path = os.path.relpath(d, parent_dir)
            print(f"   # {rel_path} æœ‰ {count} ä¸ªæ–‡ä»¶")
            print(f"   # cp {d}/*.cif {data_dir}/")

    print(f"\n" + "=" * 70)

    return current_csv_lines, current_cif_count


def main():
    import argparse

    parser = argparse.ArgumentParser(description='è¯Šæ–­æ•°æ®ä¸¢å¤±é—®é¢˜')
    parser.add_argument('data_dir', nargs='?', help='æ•°æ®ç›®å½•')

    args = parser.parse_args()

    if args.data_dir:
        data_dir = args.data_dir
    else:
        # è‡ªåŠ¨æ£€æµ‹
        if os.path.exists('./data/cif'):
            data_dir = './data/cif'
        elif os.path.exists('./data'):
            data_dir = './data'
        else:
            print("è¯·æŒ‡å®šæ•°æ®ç›®å½•")
            sys.exit(1)

    csv_lines, cif_count = diagnose_data_loss(data_dir)

    if csv_lines < 100 and cif_count < 100:
        print("\nâš ï¸  è­¦å‘Š: æ•°æ®ä¸¥é‡ä¸¢å¤±ï¼")
        print("    è¯·æŒ‰ç…§ä¸Šè¿°å»ºè®®æ¢å¤æ•°æ®")
        sys.exit(1)

    sys.exit(0)


if __name__ == "__main__":
    main()
